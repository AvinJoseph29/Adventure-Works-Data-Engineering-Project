# Adventure-Works-Data-Engineering-Project


# Azure Data Engineering Project

This project demonstrates a complete **ETL pipeline** using Azure services, including **Azure Data Lake, Azure Data Factory (ADF), Databricks, and Synapse Analytics**. The goal is to extract, transform, and load data from GitHub into a structured data warehouse, leveraging **bronze, silver, and gold layers** for efficient data processing.

![Untitled presentation (2)](https://github.com/user-attachments/assets/21c2e512-fd13-4f99-a2f0-dd04356a581a)



## ğŸ“Œ Project Overview

### ğŸ”¹ Azure Resources Used
1. ğŸ— **Resource Group** â€“ Organizes all the services under one group.
2. ğŸ’¾ **Azure Data Lake Storage** â€“ Stores raw, processed, and transformed data.
3. ğŸ”„ **Azure Data Factory (ADF)** â€“ Handles data ingestion and movement.
4. ğŸ›  **Azure Databricks** â€“ Processes data with transformations.
5. ğŸ“Š **Azure Synapse Analytics** â€“ Stores final structured data and integrates with Power BI.

---

## ğŸ— Step 1: Setting Up Azure Data Lake Storage

1. **Create a Resource Group** â€“ This groups all related Azure services together.
2. **Create an Azure Data Lake Storage Account**:
   - Navigate to **Azure Portal â†’ Storage Accounts**.
   - Click **Create Storage Account**, select your resource group, and provide a unique name.
   - Enable **Hierarchical Namespace** for optimized big data operations.
3. **Create Containers in Data Lake**:
   - Go to **Data Lake Storage Account â†’ Containers** and create:
     - ğŸŸ¤ **Bronze** â€“ Stores raw, unprocessed data.
     - âšª **Silver** â€“ Cleaned and transformed data.
     - ğŸŸ¡ **Gold** â€“ Final structured data for analytics.

ğŸ“Œ *Screenshot: Data Lake Storage Configuration*

---

## ğŸ— Step 2: Configuring Azure Data Factory (ADF)

### ğŸ”¹ Creating the Data Pipeline
1. Open **ADF Studio** â†’ **Author Tab** â†’ Click **New Pipeline**.
2. Under **Activities â†’ Move & Transform**, drag and drop the **Copy Data** activity.
3. **Configure Source and Sink**:
   - **Source:** GitHub dataset (CSV file).
   - **Sink:** Azure Data Lake (Bronze Container).

ğŸ“Œ *Screenshot: ADF Pipeline Overview*

### ğŸ”¹ Setting Up Linked Services
1. **Create a Linked Service for GitHub**:
   - Choose **HTTP Connector**.
   - Provide **Base URL**: `https://raw.githubusercontent.com/`
   - Use relative path: `anshlambaoldgit/Adventure-Works-Data-Engineering-Project/refs/heads/main/Data/AdventureWorks_Sales_2015.csv`
2. **Create a Linked Service for Data Lake**:
   - Choose **Azure Data Lake Storage Gen2**.
   - Provide authentication details.

ğŸ“Œ *Screenshot: Creating Linked Services in ADF*

### ğŸ”¹ Running the Pipeline
1. Click **Debug** to test pipeline execution.
2. Verify the file in **Bronze container** in Data Lake.
3. Click **Publish** to save the pipeline.

ğŸ“Œ *Screenshot: Data Successfully Copied to Data Lake*

---

## ğŸ— Step 3: Building Dynamic Pipelines in ADF

### ğŸ”¹ Configuring Parameters
1. Define **three parameters**:
   - `RelativeLink` â€“ Dynamic GitHub dataset link.
   - `FolderName` â€“ Folder in Data Lake.
   - `FileName` â€“ CSV file name.
2. Modify source and sink properties to accept **dynamic content**.

### ğŸ”¹ Using ForEach Activity
1. **Create a JSON file** containing dataset parameters.
2. **Upload JSON file** to Data Lake.
3. **Use Lookup Activity** to read JSON and pass it to **ForEach Activity**.
4. **Copy Data Activity** dynamically processes multiple datasets.

ğŸ“Œ *Screenshot: Dynamic Pipeline Execution in ADF*

---

## ğŸ— Step 4: Data Transformation with Azure Databricks

### ğŸ”¹ Connecting Databricks with Data Lake
1. **Create an Azure Databricks workspace**.
2. **Deploy a compute cluster**.
3. **Configure access-level permissions** in Data Lake.
4. **Generate a secret key** for authentication.
5. **Use Microsoft Documentation Code Snippet** to establish connection.

ğŸ“Œ *Screenshot: Configuring Secrets in Databricks*

### ğŸ”¹ Transforming Data (Bronze to Silver Layer)
1. **Read Bronze layer data** into a Databricks notebook.
2. **Apply transformations** (data cleaning, filtering, aggregations).
3. **Convert data to Parquet format**.
4. **Write transformed data** to the **Silver container** in Data Lake.

ğŸ“Œ *Screenshot: Transformed Data in Silver Layer*

---

## ğŸ— Step 5: Structuring Data with Azure Synapse Analytics

### ğŸ”¹ Setting Up Synapse Analytics
1. **Create an Azure Synapse Analytics workspace**.
2. **Provision a serverless SQL pool**.

### ğŸ”¹ Connecting Synapse to Data Lake
1. Use **Managed Identity Authentication** to access Data Lake.
2. Configure **IAM role assignments** in Data Lake.

### ğŸ”¹ Querying Data in Synapse
1. **Create a SQL Database** (`awdatabase`).
2. Use `OPENROWSET` to read Parquet data from **Silver Layer**.
3. Create **views** for frequently accessed datasets.

ğŸ“Œ *Screenshot: Querying Data in Synapse SQL Pool*

### ğŸ”¹ Creating External Tables
1. **Create a Database Master Key**.
2. **Define Database Scoped Credentials**.
3. **Create External Data Sources** for Silver and Gold layers.
4. **Define External File Format** for Parquet.
5. **Move data from Silver to Gold using `CETAS`**.
6. Verify structured data in **Gold container**.

ğŸ“Œ *Screenshot: External Table Creation in Synapse*

---

## ğŸ— Step 6: Visualizing Data with Power BI

### ğŸ”¹ Connecting Power BI to Synapse
1. Retrieve **SQL Endpoint** from Synapse.
2. Open **Power BI â†’ Get Data**.
3. Paste the **SQL Endpoint** to establish a connection.
4. Build **dashboards and reports**.

ğŸ“Œ *Screenshot: Power BI Dashboard Connected to Synapse*

---

## ğŸ¯ Conclusion
This project successfully implements an **end-to-end ETL pipeline** using Azure services. The structured approach ensures **scalable and efficient data processing**:
- **Raw data ingestion** from GitHub (**Bronze Layer**).
- **Data transformation** using Databricks (**Silver Layer**).
- **Data structuring & querying** in Synapse (**Gold Layer**).
- **Data visualization** in Power BI.

This setup is ideal for real-world **data engineering workflows** with automated ingestion, transformation, and reporting capabilities.
